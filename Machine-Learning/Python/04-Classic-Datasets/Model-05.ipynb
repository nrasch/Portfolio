{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#MNIST-Classification-with-a-Long-Short-Term-Memory-(LSTM)-network\" data-toc-modified-id=\"MNIST-Classification-with-a-Long-Short-Term-Memory-(LSTM)-network-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>MNIST Classification with a Long Short Term Memory (LSTM) network</a></span></li><li><span><a href=\"#Init-vars\" data-toc-modified-id=\"Init-vars-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Init vars</a></span></li><li><span><a href=\"#Build-the-computational-graph\" data-toc-modified-id=\"Build-the-computational-graph-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Build the computational graph</a></span><ul class=\"toc-item\"><li><span><a href=\"#ELU-model\" data-toc-modified-id=\"ELU-model-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>ELU model</a></span></li><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Result</a></span></li></ul></li><li><span><a href=\"#TensorBoard---Second-Run\" data-toc-modified-id=\"TensorBoard---Second-Run-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>TensorBoard - Second Run</a></span><ul class=\"toc-item\"><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Result</a></span></li></ul></li><li><span><a href=\"#TensorBoard---Third-Run\" data-toc-modified-id=\"TensorBoard---Third-Run-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>TensorBoard - Third Run</a></span><ul class=\"toc-item\"><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Result</a></span></li></ul></li><li><span><a href=\"#TensorBoard---Fourth-Run\" data-toc-modified-id=\"TensorBoard---Fourth-Run-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>TensorBoard - Fourth Run</a></span><ul class=\"toc-item\"><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Result</a></span></li></ul></li><li><span><a href=\"#TensorBoard---Fifth-Run\" data-toc-modified-id=\"TensorBoard---Fifth-Run-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>TensorBoard - Fifth Run</a></span><ul class=\"toc-item\"><li><span><a href=\"#Result\" data-toc-modified-id=\"Result-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Result</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MNIST classification with a Long Short Term Memory (LSTM) network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:08:44.796666Z",
     "start_time": "2018-09-18T21:08:44.793666Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:08:44.914666Z",
     "start_time": "2018-09-18T21:08:44.798666Z"
    }
   },
   "outputs": [],
   "source": [
    "def resetGraph(seed= 10):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:08:45.153666Z",
     "start_time": "2018-09-18T21:08:44.915666Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleanLogs():\n",
    "    os.system('rm -rf ./logs/dnnTensorBoard/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init vars\n",
    "\n",
    "The LSTM wants inputs of shape `[samples, timeSteps, features]`, and we have several thousand MNIST images of size 28 x 28 pixels.  \n",
    "\n",
    "One way to think of this is a complete image is comprised of 28 rows of 28 pixels each.  If we were to step through the rows one by one and stack them up then the image would be more and more complete as time went by.  So our units of \"time\" will be the rows stacking together to create a complete image, and the number of features will be the number of pixels in the image row at that step in time (i.e. 28).  This gives us:\n",
    "\n",
    "* samples     = number of observations (i.e. number of images in the mini batch)\n",
    "* timeSteps   = number of rows we need to step through/stack up to make a complete image\n",
    "* features    = the number of features in each row we are stepping through (i.e. also 28)\n",
    "\n",
    "Additionally, we only care about the final output of the LSTM network which should give us the prediction of which numeral the image represents.  Other LSTM networks do care about the outputs of each LSTM cell (translating each word in a sentence for example), but that doesn't apply in our case.\n",
    "\n",
    "Having said this we can continue with initializing the various variables we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:10:06.564666Z",
     "start_time": "2018-09-18T21:10:06.098666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "\n",
      " Example label:  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Setup vars for the MINST data set\n",
    "timeSteps = 28\n",
    "features = 28\n",
    "\n",
    "lstmUnits = 128\n",
    "lr = 0.001\n",
    "epochs = 10\n",
    "samples = 50\n",
    "\n",
    "classes = 10\n",
    "\n",
    "# Notice we are pulling in the labels as one hot encodings!\n",
    "mninst = input_data.read_data_sets(\"./datasets/mnist\", one_hot = True)\n",
    "\n",
    "# For use when we create the LSTM network below\n",
    "testShape = mninst.test.images.shape\n",
    "\n",
    "# Note the one hot encoding on the label:\n",
    "print(\"\\n\", \"Example label: \", mninst.test.labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-07T20:47:13.613722Z",
     "start_time": "2018-09-07T20:47:13.609737Z"
    }
   },
   "source": [
    "# Build the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:15:16.094666Z",
     "start_time": "2018-09-18T21:15:14.061666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Clean away any old log files\n",
    "cleanLogs()\n",
    "\n",
    "# Set the TB logdir - We want two log dirs since we are going to be plotting two values on the same plot\n",
    "logDirTrain = './logs/mnistLSTM/runOne/train'\n",
    "logDirTest = './logs/mnistLSTM/runOne/test'\n",
    "\n",
    "\n",
    "# Create place holders\n",
    "x = tf.placeholder(tf.float32, shape = [None, timeSteps, features], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None, classes], name = 'y')\n",
    "\n",
    "# Create weights and bias tensors\n",
    "with tf.name_scope(\"weightBias\"):\n",
    "    # Todo: Better init!!\n",
    "    w = tf.Variable(tf.random_normal([lstmUnits, classes]))\n",
    "    b = tf.Variable(tf.random_normal([classes]))\n",
    "\n",
    "\n",
    "# Add the LSTM cells\n",
    "with tf.name_scope(\"LSTM\"):\n",
    "    \n",
    "    # Later in the code we'll make a call to tf.contrib.rnn.static_rnn\n",
    "    # tf.contrib.rnn.static_rnn expects a length T list of inputs, each a Tensor of shape [batch_size, input_size]\n",
    "    # So we need to convert our inputs of shape [batchSize, timeSteps, numberOfInputs] to [batch_size, input_size]\n",
    "    #\n",
    "    # https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/rnn/static_rnn\n",
    "    \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/unstack\n",
    "    inputs = tf.unstack(x, num = timeSteps, axis = 1)\n",
    "    \n",
    "    # Create the basic LSTM cell\n",
    "    # It does not allow cell clipping, a projection layer, and does not use peep-hole connections: it is the basic baseline.\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "    \n",
    "    # Add the cell to the RNN\n",
    "    # https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/rnn/static_rnn\n",
    "    output, state = tf.contrib.rnn.static_rnn(cell, inputs, dtype = tf.float32)\n",
    "    \n",
    "    # We only care about the final output which should be the model's prediction\n",
    "    yH = tf.matmul(output[-1], w) + b\n",
    "    \n",
    "# Add loss function\n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(logits = yH, labels = y)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    # Capture loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt = tf.train.AdamOptimizer(learning_rate = lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.equal(tf.argmax(yH, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    # Capture accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T21:32:47.556966Z",
     "start_time": "2018-09-18T21:28:10.774141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.96 Test Acc:  0.9614\n",
      "10 Train Acc:  1.0 Test Acc:  0.984\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.984\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "counter = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Create the TB writer and init\n",
    "    trainWriter = tf.summary.FileWriter(logDirTrain, sess.graph)\n",
    "    testWriter = tf.summary.FileWriter(logDirTest)\n",
    "    merge = tf.summary.merge_all()\n",
    "    \n",
    "    for e in range(epochs + 1):\n",
    "        for i in range(mninst.train.num_examples // samples):      \n",
    "            counter += 1     \n",
    "            \n",
    "            # Grab the next minibatch\n",
    "            xBatch, yBatch = mninst.train.next_batch(samples)\n",
    "            \n",
    "            # Reshape x to [samples, timeSteps, features] for the LSTM:\n",
    "            #   The image is given to us as a single vector of dimensionality 784\n",
    "            #   So to use them we need to gather the number of rows together to be the timeSteps\n",
    "            xBatch = xBatch.reshape(samples, timeSteps, features)\n",
    "            \n",
    "            # Train the model\n",
    "            summary, _ = sess.run([merge, opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            \n",
    "            # Capture summary data every N steps\n",
    "            if counter % 10 == 0:\n",
    "                # Manually add to the train accuracy summary value\n",
    "                summary, accTrain = sess.run([merge, accuracy], feed_dict = {x: xBatch, y: yBatch})\n",
    "                trainWriter.add_summary(summary, counter) \n",
    "                \n",
    "                # Manually add to the test accuracy summary value\n",
    "                #summary, accTest = sess.run([merge, accuracy], feed_dict = {\n",
    "                #    x: mninst.test.images[:1450].reshape(-1, timeSteps, features), \n",
    "                #    y: mninst.test.labels[:1450]})\n",
    "                summary, accTest = sess.run([merge, accuracy], feed_dict = {\n",
    "                    x: mninst.test.images.reshape(-1, timeSteps, features), \n",
    "                    y: mninst.test.labels})\n",
    "                testWriter.add_summary(summary, counter)\n",
    "                \n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
