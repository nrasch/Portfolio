{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Purpose</a></span></li><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Setup</a></span></li><li><span><a href=\"#Vanishing/Exploding-Gradients\" data-toc-modified-id=\"Vanishing/Exploding-Gradients-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Vanishing/Exploding Gradients</a></span><ul class=\"toc-item\"><li><span><a href=\"#Xavier-initializer-(aka-Glorot-uniform-initializer)\" data-toc-modified-id=\"Xavier-initializer-(aka-Glorot-uniform-initializer)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Xavier initializer (aka Glorot uniform initializer)</a></span></li><li><span><a href=\"#He-initializer\" data-toc-modified-id=\"He-initializer-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>He initializer</a></span></li><li><span><a href=\"#Nonsaturating-Activation-Functions\" data-toc-modified-id=\"Nonsaturating-Activation-Functions-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Nonsaturating Activation Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exponential-linear-unit-(ELU)\" data-toc-modified-id=\"Exponential-linear-unit-(ELU)-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Exponential linear unit (ELU)</a></span></li><li><span><a href=\"#Leaky-ReLu\" data-toc-modified-id=\"Leaky-ReLu-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Leaky ReLu</a></span></li></ul></li><li><span><a href=\"#Batch-normalization\" data-toc-modified-id=\"Batch-normalization-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Batch normalization</a></span></li><li><span><a href=\"#Gradient-Clipping\" data-toc-modified-id=\"Gradient-Clipping-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Gradient Clipping</a></span></li></ul></li><li><span><a href=\"#Transfer-Learning\" data-toc-modified-id=\"Transfer-Learning-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Transfer Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-sample-model-to-learn-from\" data-toc-modified-id=\"Build-sample-model-to-learn-from-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Build sample model to learn from</a></span></li><li><span><a href=\"#Transfer-learning---Method-one\" data-toc-modified-id=\"Transfer-learning---Method-one-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Transfer learning - Method one</a></span></li><li><span><a href=\"#Transfer-learning---Method-two\" data-toc-modified-id=\"Transfer-learning---Method-two-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Transfer learning - Method two</a></span></li><li><span><a href=\"#Freezing-the-lower-layers\" data-toc-modified-id=\"Freezing-the-lower-layers-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Freezing the lower layers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Method-one---tf.get_collection\" data-toc-modified-id=\"Method-one---tf.get_collection-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Method one - tf.get_collection</a></span></li><li><span><a href=\"#Method-two---tf.stop_gradient\" data-toc-modified-id=\"Method-two---tf.stop_gradient-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Method two - tf.stop_gradient</a></span></li></ul></li><li><span><a href=\"#Caching-frozen-layers\" data-toc-modified-id=\"Caching-frozen-layers-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Caching frozen layers</a></span></li><li><span><a href=\"#Altering-layers\" data-toc-modified-id=\"Altering-layers-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Altering layers</a></span></li></ul></li><li><span><a href=\"#Optimizers\" data-toc-modified-id=\"Optimizers-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Optimizers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Momentum\" data-toc-modified-id=\"Momentum-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Momentum</a></span></li><li><span><a href=\"#Momentum-with-exponential-learning-rate-scheduling\" data-toc-modified-id=\"Momentum-with-exponential-learning-rate-scheduling-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Momentum with exponential learning rate scheduling</a></span></li><li><span><a href=\"#Nesterov-Accelerated-Gradient\" data-toc-modified-id=\"Nesterov-Accelerated-Gradient-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Nesterov Accelerated Gradient</a></span></li><li><span><a href=\"#RMSProp\" data-toc-modified-id=\"RMSProp-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>RMSProp</a></span></li><li><span><a href=\"#Adam\" data-toc-modified-id=\"Adam-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Adam</a></span></li></ul></li><li><span><a href=\"#Regularization\" data-toc-modified-id=\"Regularization-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Regularization</a></span><ul class=\"toc-item\"><li><span><a href=\"#$\\ell_1$-and-$\\ell_2$-regularization\" data-toc-modified-id=\"$\\ell_1$-and-$\\ell_2$-regularization-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>$\\ell_1$ and $\\ell_2$ regularization</a></span></li><li><span><a href=\"#Dropout\" data-toc-modified-id=\"Dropout-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Dropout</a></span></li><li><span><a href=\"#Max-Norm-Regularization\" data-toc-modified-id=\"Max-Norm-Regularization-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Max-Norm Regularization</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "<img style=\"float: left; margin-right: 15px;\" height = 40%; width = 40%; src=\"./images/TensorFlow_banner.png\" />\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "This is a strictly technical notebook; minimal narrative has been added other than code comments or explanations where deemed required.\n",
    "\n",
    "The goal of this write-up is to create a number of TensorFlow (TF) models each implementing a different architecture for reference purposes.\n",
    "\n",
    "Configuration options covered include:\n",
    "* Gradients\n",
    "* Activation functions\n",
    "* Transfer learning\n",
    "* Various optimizers\n",
    "* Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:16:47.837591Z",
     "start_time": "2018-09-10T01:16:47.832590Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:16:49.170664Z",
     "start_time": "2018-09-10T01:16:48.004580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Setup vars for the MINST data set\n",
    "inputs = 28 * 28    # image dim in pixels\n",
    "hidden1 = 300\n",
    "hidden2 = 100\n",
    "hidden3 = 50\n",
    "hidden4 = 50\n",
    "hidden5 = 50\n",
    "outputs = 10\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "batchSize = 50\n",
    "\n",
    "mninst = input_data.read_data_sets(\"./datasets/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetGraph( seed= 10):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing/Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-07T20:47:08.055767Z",
     "start_time": "2018-09-07T20:47:08.051832Z"
    }
   },
   "source": [
    "## Xavier initializer (aka Glorot uniform initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the default init for the `tf.layers.dense` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-07T20:47:13.613722Z",
     "start_time": "2018-09-07T20:47:13.609737Z"
    }
   },
   "source": [
    "## He initializer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:12:57.334476Z",
     "start_time": "2018-09-09T05:12:57.328458Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:12:57.873940Z",
     "start_time": "2018-09-09T05:12:57.662931Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:23.776171Z",
     "start_time": "2018-09-09T05:12:59.911367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.92 Test Acc:  0.9055\n",
      "10 Train Acc:  0.98 Test Acc:  0.9605\n",
      " \n",
      "FINAL ::  Train Acc:  0.98 Test Acc:  0.9699\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helps to mitigate the `dying ReLu` problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential linear unit (ELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:53.014680Z",
     "start_time": "2018-09-09T05:14:52.987684Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "entropy = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "resetGraph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:14:54.082231Z",
     "start_time": "2018-09-09T05:14:53.792180Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    # Use ELU\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.elu)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.elu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:16:23.114258Z",
     "start_time": "2018-09-09T05:14:56.169295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.9 Test Acc:  0.9023\n",
      "10 Train Acc:  0.96 Test Acc:  0.949\n",
      " \n",
      "FINAL ::  Train Acc:  0.98 Test Acc:  0.9625\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:16:23.132242Z",
     "start_time": "2018-09-09T05:16:23.117241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "entropy = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "resetGraph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:16:23.213247Z",
     "start_time": "2018-09-09T05:16:23.174245Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write the leaky ReLu function\n",
    "def leakyReLu(z, name = None):\n",
    "    return tf.maximum(0.01 * z, z, name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:16:23.568265Z",
     "start_time": "2018-09-09T05:16:23.216247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    # Use leaky ReLu\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = leakyReLu)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = leakyReLu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:17:53.829453Z",
     "start_time": "2018-09-09T05:16:23.573285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.94 Test Acc:  0.9041\n",
      "10 Train Acc:  1.0 Test Acc:  0.9595\n",
      " \n",
      "FINAL ::  Train Acc:  0.98 Test Acc:  0.9714\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:17:53.852456Z",
     "start_time": "2018-09-09T05:17:53.832453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "entropy = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "resetGraph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:17:53.883446Z",
     "start_time": "2018-09-09T05:17:53.855456Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write a partial, so we can have cleaner code  :)\n",
    "from functools import partial\n",
    "\n",
    "# We'll use this to control batch normalization during training... \n",
    "# Set it to TRUE when training, and FALSE otherwise\n",
    "training = tf.placeholder_with_default(False, shape = (), name = 'training')\n",
    "\n",
    "batchNormLayer = partial(tf.layers.batch_normalization, training = training, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:17:54.964502Z",
     "start_time": "2018-09-09T05:17:53.889439Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\")\n",
    "    batchNorm1 = batchNormLayer(layer1)\n",
    "    batchNorm1Activation = tf.nn.elu(batchNorm1)\n",
    "    \n",
    "    layer2 = tf.layers.dense(batchNorm1Activation, hidden2, name = \"hLayerTwo\")\n",
    "    batchNorm2 = batchNormLayer(layer2)\n",
    "    batchNorm2Activation = tf.nn.elu(batchNorm2)\n",
    "    \n",
    "    \n",
    "    yHBeforeBatchNorm = tf.layers.dense(batchNorm2Activation, outputs, name = \"yH\")\n",
    "    yH = batchNormLayer(yHBeforeBatchNorm)   \n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:19:53.042264Z",
     "start_time": "2018-09-09T05:17:54.975503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  1.0 Test Acc:  0.925\n",
      "10 Train Acc:  1.0 Test Acc:  0.9744\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9779\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "\n",
    "# New operations added from the batch normalization\n",
    "updateOps = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            # Note that training = TRUE\n",
    "            sess.run([opt, updateOps], feed_dict = {training: True, x: xBatch, y: yBatch})\n",
    "        # Note that training = FALSE by default\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:19:53.078283Z",
     "start_time": "2018-09-09T05:19:53.045263Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "entropy = batchNorm1 = batchNorm1Activation = batchNorm2 = batchNorm2Activation = None\n",
    "yHBeforeBatchNorm = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "resetGraph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:19:53.430303Z",
     "start_time": "2018-09-09T05:19:53.081283Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    # Gradient clipping threshold\n",
    "    threshold = 1.0\n",
    "    \n",
    "    # Setup optimizer to use gradient clipping\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr)\n",
    "    gradVars = opt.compute_gradients(loss)\n",
    "    cappedGradVars = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "                     for grad, var in gradVars]\n",
    "    clippedOpt = opt.apply_gradients(cappedGradVars)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Save the trained model parameters to disk\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:21:29.025776Z",
     "start_time": "2018-09-09T05:19:53.433303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.88 Test Acc:  0.9012\n",
      "10 Train Acc:  0.96 Test Acc:  0.9592\n",
      " \n",
      "FINAL ::  Train Acc:  0.98 Test Acc:  0.9708\n"
     ]
    }
   ],
   "source": [
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([clippedOpt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "    \n",
    "    # Save the trained model\n",
    "    savePath = saver.save(sess, \"./DNN-TensorFlow-Gradient-Clipping.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build sample model to learn from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T22:35:01.589522Z",
     "start_time": "2018-09-09T22:35:01.578454Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T22:36:35.145813Z",
     "start_time": "2018-09-09T22:35:01.897473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.98 Test Acc:  0.895\n",
      "10 Train Acc:  1.0 Test Acc:  0.9703\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9748\n"
     ]
    }
   ],
   "source": [
    "# Create and save base model we want to use later on for transfer learning\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    layer3 = tf.layers.dense(layer2, hidden3, name = \"hLayerThree\", activation = tf.nn.relu)\n",
    "    layer4 = tf.layers.dense(layer3, hidden4, name = \"hLayerFour\", activation = tf.nn.relu)\n",
    "    layer5 = tf.layers.dense(layer4, hidden5, name = \"hLayerFive\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer5, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'accuracy')\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Save our trained model parameters to disk\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Collect all the model operations in one spot to support model transfer learning\n",
    "for op in (x, y, layer1, layer2, layer3, layer4, layer5, yH, loss, opt, correct, accuracy):\n",
    "    tf.add_to_collection(\"modelOps\", op)\n",
    "\n",
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "    \n",
    "    # Save the CG, so we can restore and use it later on in another model\n",
    "    savePath = saver.save(sess, \"./DNN-Base-Pretrained-Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIME TAKEN:  1M 33.2S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T22:36:35.154795Z",
     "start_time": "2018-09-09T22:36:35.148813Z"
    }
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning - Method one\n",
    "\n",
    "Attach to the TF CG components by utilizing the collection created by the orig. model author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T23:21:30.353130Z",
     "start_time": "2018-09-09T23:21:30.335127Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "resetGraph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T23:21:30.956161Z",
     "start_time": "2018-09-09T23:21:30.828154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./DNN-Base-Pretrained-Model\n"
     ]
    }
   ],
   "source": [
    "sess2 = tf.Session() \n",
    "\n",
    "#First let's load meta graph and restore weights\n",
    "saver2 = tf.train.import_meta_graph('./DNN-Base-Pretrained-Model.meta')\n",
    "saver2.restore(sess2,tf.train.latest_checkpoint('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T23:22:31.226613Z",
     "start_time": "2018-09-09T23:22:31.220596Z"
    }
   },
   "outputs": [],
   "source": [
    "# Attach to the TF CG components by utilizing the collection created by the orig. model author\n",
    "# and pull out the first four\n",
    "tf.get_collection(\"modelOps\")\n",
    "x2, y2 = tf.get_collection(\"modelOps\")[0:2]\n",
    "opt2 = tf.get_collection(\"modelOps\")[9]\n",
    "accuracy2 = tf.get_collection(\"modelOps\")[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T23:22:51.579779Z",
     "start_time": "2018-09-09T23:22:51.376767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9756\n"
     ]
    }
   ],
   "source": [
    "accTest2 = accuracy2.eval(session = sess2, feed_dict = {x2: mninst.test.images, y2: mninst.test.labels})\n",
    "print(accTest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T23:23:07.092650Z",
     "start_time": "2018-09-09T23:23:07.084667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'x:0' shape=(?, 784) dtype=float32>,\n",
       " <tf.Tensor 'y:0' shape=(?,) dtype=int64>,\n",
       " <tf.Tensor 'neuralNetEasy/hLayerOne/Relu:0' shape=(?, 300) dtype=float32>,\n",
       " <tf.Tensor 'neuralNetEasy/hLayerTwo/Relu:0' shape=(?, 100) dtype=float32>,\n",
       " <tf.Tensor 'neuralNetEasy/hLayerThree/Relu:0' shape=(?, 50) dtype=float32>,\n",
       " <tf.Tensor 'neuralNetEasy/hLayerFour/Relu:0' shape=(?, 50) dtype=float32>,\n",
       " <tf.Tensor 'neuralNetEasy/hLayerFive/Relu:0' shape=(?, 50) dtype=float32>,\n",
       " <tf.Tensor 'neuralNetEasy/yH/BiasAdd:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'loss/loss:0' shape=() dtype=float32>,\n",
       " <tf.Operation 'optimizer/GradientDescent' type=NoOp>,\n",
       " <tf.Tensor 'eval/in_top_k/InTopKV2:0' shape=(?,) dtype=bool>,\n",
       " <tf.Tensor 'eval/accuracy:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the items in the collection\n",
    "tf.get_collection(\"modelOps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning - Method two\n",
    "\n",
    "Attach to the TF CG components by calling them specificly from the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T23:23:07.948719Z",
     "start_time": "2018-09-09T23:23:07.931697Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "resetGraph()\n",
    "\n",
    "# Confirm everything is cleared out\n",
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T23:23:08.483748Z",
     "start_time": "2018-09-09T23:23:08.323740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./DNN-Base-Pretrained-Model\n"
     ]
    }
   ],
   "source": [
    "# Create a new session\n",
    "sess = tf.Session()    \n",
    "\n",
    "#First let's load meta graph and restore weights\n",
    "saver = tf.train.import_meta_graph('./DNN-Base-Pretrained-Model.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T23:23:09.024779Z",
     "start_time": "2018-09-09T23:23:08.783746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9756\n"
     ]
    }
   ],
   "source": [
    "# Attach to the TF CG components by calling them specifically from the graph\n",
    "graph = tf.get_default_graph()\n",
    "x = graph.get_tensor_by_name(\"x:0\")\n",
    "y = graph.get_tensor_by_name(\"y:0\")\n",
    "\n",
    "feed_dict = {x: mninst.test.images, y: mninst.test.labels}\n",
    "accuracy = graph.get_tensor_by_name(\"eval/accuracy:0\")\n",
    "\n",
    "accTest = accuracy.eval(session = sess, feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "print(accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:10.590576Z",
     "start_time": "2018-09-09T05:23:10.585594Z"
    }
   },
   "source": [
    "## Freezing the lower layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:34:28.310319Z",
     "start_time": "2018-09-10T01:34:28.306300Z"
    }
   },
   "source": [
    "### Method one - tf.get_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:25:19.436888Z",
     "start_time": "2018-09-10T01:25:19.411868Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "resetGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:25:20.628946Z",
     "start_time": "2018-09-10T01:25:20.491930Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load previous model meta graph\n",
    "saver = tf.train.import_meta_graph('./DNN-Base-Pretrained-Model.meta')\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:25:22.940070Z",
     "start_time": "2018-09-10T01:25:22.931084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hLayerFour/kernel:0' shape=(50, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'hLayerFour/bias:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'hLayerFive/kernel:0' shape=(50, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'hLayerFive/bias:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'yH/kernel:0' shape=(50, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'yH/bias:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attach to hidden layers 4, 5, and the output (yH), \n",
    "# so we can train only these layers and exclude the others (i.e. the \"frozen layers\")\n",
    "trainableVars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = \"hLayerFour|hLayerFive|yH\")\n",
    "trainableVars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:26:25.232659Z",
     "start_time": "2018-09-10T01:25:26.508275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./DNN-Base-Pretrained-Model\n",
      "0 Train Acc:  0.98 Test Acc:  0.977\n",
      "10 Train Acc:  1.0 Test Acc:  0.9761\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9761\n"
     ]
    }
   ],
   "source": [
    "# Attach to the TF CG components by calling them specifically from the graph\n",
    "x = graph.get_tensor_by_name(\"x:0\")\n",
    "y = graph.get_tensor_by_name(\"y:0\")\n",
    "loss = graph.get_tensor_by_name(\"loss/loss:0\")\n",
    "accuracy = graph.get_tensor_by_name(\"eval/accuracy:0\")\n",
    "\n",
    "# Create a new optimizer, and pass it the unfrozen layers we pulled out in the previous step\n",
    "opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss, var_list = trainableVars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Restore weights from orig. model\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "    \n",
    "    # Further training using new layers and/or new data\n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIME TAKEN:  58.7s  \n",
    "ORIG MODEL TIME TAKEN: 1M 33.2S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:26:25.240656Z",
     "start_time": "2018-09-10T01:26:25.235636Z"
    }
   },
   "source": [
    "### Method two - tf.stop_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:26:25.316659Z",
     "start_time": "2018-09-10T01:26:25.243636Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "resetGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:26:25.674680Z",
     "start_time": "2018-09-10T01:26:25.319652Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load previous model meta graph\n",
    "saver = tf.train.import_meta_graph('./DNN-Base-Pretrained-Model.meta')\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-10T01:27:23.764006Z",
     "start_time": "2018-09-10T01:26:25.677663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./DNN-Base-Pretrained-Model\n",
      "0 Train Acc:  0.96 Test Acc:  0.964\n",
      "10 Train Acc:  1.0 Test Acc:  0.974\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9752\n"
     ]
    }
   ],
   "source": [
    "# Attach to the TF CG components by calling them specifically from the graph\n",
    "x = graph.get_tensor_by_name(\"x:0\")\n",
    "y = graph.get_tensor_by_name(\"y:0\")\n",
    "l3 = graph.get_tensor_by_name(\"neuralNetEasy/hLayerThree/Relu:0\")\n",
    "\n",
    "# Freeze the network below this point\n",
    "l3Stop = tf.stop_gradient(l3)\n",
    "\n",
    "# Add some new, unfrozen layers to the CG\n",
    "newL4 = tf.layers.dense(l3Stop, hidden4, name = \"newHLayerFour\", activation = tf.nn.relu)\n",
    "newL5 = tf.layers.dense(newL4, hidden5, name = \"newHLayerFive\", activation = tf.nn.relu)\n",
    "newYH = tf.layers.dense(newL5, outputs, name = \"newYH\")\n",
    "\n",
    "# Define new loss function\n",
    "with tf.name_scope(\"newLoss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = newYH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "       \n",
    "# New eval function\n",
    "with tf.name_scope(\"newEval\"):\n",
    "    correct = tf.nn.in_top_k(newYH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'accuracy')\n",
    "    \n",
    "# New optimizer\n",
    "with tf.name_scope(\"newOptimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Init vars\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Restore weights from orig. model\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "    \n",
    "    # Further training using new layers and/or new data\n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIME TAKEN:  58.1s  \n",
    "ORIG MODEL TIME TAKEN: 1M 33.2S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:10.650597Z",
     "start_time": "2018-09-09T05:23:10.593594Z"
    }
   },
   "source": [
    "## Caching frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear everything out\n",
    "x = y = layer1 = layer2 = yH = entropy = loss = opt = correct = None\n",
    "accuracy = init = saver = accTrain = accTest = savePath = sess = None\n",
    "l3 = l3Stop = newL4 = newL5 = newYH = None\n",
    "\n",
    "# Clear away all our previous work\n",
    "resetGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous model meta graph\n",
    "saver = tf.train.import_meta_graph('./DNN-Base-Pretrained-Model.meta')\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./DNN-Base-Pretrained-Model\n",
      "0 Train Acc:  1.0 Test Acc:  0.9648\n",
      "10 Train Acc:  0.98 Test Acc:  0.9753\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9751\n"
     ]
    }
   ],
   "source": [
    "# Attach to the TF CG components by calling them specifically from the graph\n",
    "x = graph.get_tensor_by_name(\"x:0\")\n",
    "y = graph.get_tensor_by_name(\"y:0\")\n",
    "\n",
    "# Cache this layer\n",
    "l3 = graph.get_tensor_by_name(\"neuralNetEasy/hLayerThree/Relu:0\")\n",
    "\n",
    "# Don't forget this part!  We don't want the model using any of the earlier hidden layers....\n",
    "l3Stop = tf.stop_gradient(l3)\n",
    "\n",
    "# Add some new, unfrozen layers to the CG\n",
    "newL4 = tf.layers.dense(l3Stop, hidden4, name = \"newHLayerFour\", activation = tf.nn.relu)\n",
    "newL5 = tf.layers.dense(newL4, hidden5, name = \"newHLayerFive\", activation = tf.nn.relu)\n",
    "newYH = tf.layers.dense(newL5, outputs, name = \"newYH\")\n",
    "\n",
    "# Define new loss function\n",
    "with tf.name_scope(\"newLoss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = newYH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "       \n",
    "# New eval function\n",
    "with tf.name_scope(\"newEval\"):\n",
    "    correct = tf.nn.in_top_k(newYH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name = 'accuracy')\n",
    "    \n",
    "# New optimizer\n",
    "with tf.name_scope(\"newOptimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # Restore weights from orig. model\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "    \n",
    "    # Create the cache of hidden layer three\n",
    "    # We run the test and train data through the previously trained model layer\n",
    "    # And cache the result, so we don't have to run through this again\n",
    "    l3CacheTrain = sess.run(l3, feed_dict = {x: mninst.train.images, y: mninst.train.labels})\n",
    "    l3CacheTest = sess.run(l3, feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "    \n",
    "    numCacheGroupings = len(l3CacheTrain) // batchSize\n",
    "    \n",
    "    # Further training using new layers and/or new data with the L3 cache\n",
    "    for e in range(epochs):\n",
    "        # Split the L3 cache up into batches\n",
    "        indices = np.random.permutation(len(mninst.train.images))\n",
    "        cacheBatchesData = np.array_split(l3CacheTrain[indices], numCacheGroupings)\n",
    "        cacheBatchLabels = np.array_split(mninst.train.labels[indices], numCacheGroupings)\n",
    "              \n",
    "        for xBatch, yBatch in zip(cacheBatchesData, cacheBatchLabels):\n",
    "            \n",
    "            # Note we pass in the cached layer to the optimizer instad of the training data\n",
    "            sess.run([opt], feed_dict = {l3: xBatch, y: yBatch})\n",
    "            \n",
    "            accTrain = accuracy.eval(feed_dict = {l3: xBatch, y: yBatch})\n",
    "            accTest = accuracy.eval(feed_dict = {l3: l3CacheTest, y: mninst.test.labels})\n",
    "        \n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-09T05:23:10.701582Z",
     "start_time": "2018-09-09T05:23:10.654598Z"
    }
   },
   "source": [
    "## Altering layers\n",
    "\n",
    "See the above section on `Transfer Learning` for examples of this where we added additional layers, estimators, and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "\n",
    "Note the hyperparameter `momentum` can by tuned for this model.\n",
    "\n",
    "[API link](https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.94 Test Acc:  0.9527\n",
      "10 Train Acc:  1.0 Test Acc:  0.9813\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9825\n"
     ]
    }
   ],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.MomentumOptimizer(learning_rate = lr, momentum = 0.9).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum with exponential learning rate scheduling\n",
    "\n",
    "[Optimizer API link](https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer)\n",
    "\n",
    "[tf.train.exponential_decay API link](https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.94 Test Acc:  0.9568\n",
      "10 Train Acc:  1.0 Test Acc:  0.9756\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9703\n"
     ]
    }
   ],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "####\n",
    "# Set up the learning exponential learning rate scheduling\n",
    "initialLR = 0.1\n",
    "decaySteps = 10000\n",
    "decayRate = .1\n",
    "\n",
    "# Create a non-trainable TF var to track which training epoch we are on\n",
    "globalStep = tf.Variable(0, trainable = False, name = \"globalStep\")\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay\n",
    "scheduledLearningRate = tf.train.exponential_decay(initialLR, globalStep, decaySteps, decayRate)\n",
    "\n",
    "# Define the optimizer\n",
    "_opt =  tf.train.MomentumOptimizer(learning_rate = scheduledLearningRate, momentum = 0.9)\n",
    "####\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt = _opt.minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient\n",
    "\n",
    "Note the hyperparameter `momentum` can by tuned for this model.\n",
    "\n",
    "[API link](https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.94 Test Acc:  0.9528\n",
      "10 Train Acc:  1.0 Test Acc:  0.9809\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9824\n"
     ]
    }
   ],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.MomentumOptimizer(learning_rate = lr, momentum = 0.9, use_nesterov = True).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp\n",
    "\n",
    "[API link](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.86 Test Acc:  0.9635\n",
      "10 Train Acc:  1.0 Test Acc:  0.9769\n",
      " \n",
      "FINAL ::  Train Acc:  1.0 Test Acc:  0.9816\n"
     ]
    }
   ],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    # Note the use of a smaller learning rate\n",
    "    opt =  tf.train.RMSPropOptimizer(learning_rate = 0.0001, momentum = 0.9, decay = 0.9, epsilon = 1e-10).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "[API link](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/train/AdamOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  1.0 Test Acc:  0.9484\n",
      "10 Train Acc:  1.0 Test Acc:  0.9603\n",
      " \n",
      "FINAL ::  Train Acc:  0.96 Test Acc:  0.9623\n"
     ]
    }
   ],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    # Note the use of a smaller learning rate\n",
    "    opt =  tf.train.AdamOptimizer(learning_rate = lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_1$ and $\\ell_2$ regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.78 Test Acc:  0.8989\n",
      "10 Train Acc:  1.0 Test Acc:  0.9196\n",
      " \n",
      "FINAL ::  Train Acc:  0.94 Test Acc:  0.9293\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# Init L1 regularization scale value\n",
    "l1Scale = 0.001\n",
    "\n",
    "# Wrap layer creation with regularization in a partial\n",
    "makeDense = partial(\n",
    "    tf.layers.dense, \n",
    "    activation = tf.nn.relu,\n",
    "    kernel_regularizer = tf.contrib.layers.l1_regularizer(l1Scale)\n",
    ")\n",
    "####\n",
    "\n",
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    # Note below we add the layers with L1 regularization using the partial defined above\n",
    "    # (We could also utilize L2 or L1 + L2)\n",
    "    layer1 = makeDense(x, hidden1, name = \"hLayerOne\", kernel_initializer = heInit)\n",
    "    layer2 = makeDense(layer1, hidden2, name = \"hLayerTwo\")\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    \n",
    "    # We also need to update the loss function for the addition of regularization\n",
    "    # Otherwise the regularization will be ignored\n",
    "    baseLoss = tf.reduce_mean(entropy, name = \"avgEntropy\")\n",
    "    regLoss = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    # Orig loss function --->  loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    loss = tf.add_n([baseLoss] + regLoss, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "[API link](https://www.tensorflow.org/api_docs/python/tf/layers/dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.9 Test Acc:  0.89\n",
      "10 Train Acc:  0.94 Test Acc:  0.9567\n",
      " \n",
      "FINAL ::  Train Acc:  0.96 Test Acc:  0.9693\n"
     ]
    }
   ],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "####\n",
    "# Init dropout rate --> Drop out 20% of the inputs\n",
    "dropRate = 0.2\n",
    "\n",
    "# Track if we are training or not.  Only use dropout when training the model.\n",
    "isTraining = tf.placeholder_with_default(False, shape = (), name = \"isTraining\")\n",
    "####\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    # Implement drop out at the input layer as well as in the hidden layer(s)\n",
    "    # Input dropout\n",
    "    dropInput = tf.layers.dropout(x, dropRate, training = isTraining)\n",
    "    \n",
    "    # Hidden layer with dropout\n",
    "    layer1 = tf.layers.dense(dropInput, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, kernel_initializer = heInit)\n",
    "    dropLayer1 = tf.layers.dropout(layer1, dropRate, training = isTraining)\n",
    "    \n",
    "    # Hidden layer dropout\n",
    "    layer2 = tf.layers.dense(dropLayer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu)\n",
    "    dropLayer2 = tf.layers.dropout(layer2, dropRate, training = isTraining)\n",
    "    \n",
    "    yH = tf.layers.dense(dropLayer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            # Note that training = TRUE --> dropout will occur\n",
    "            sess.run([opt], feed_dict = {isTraining: True, x: xBatch, y: yBatch})\n",
    "        \n",
    "        # Note that training = FALSE by default --> dropout will not occur\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a max-norm reg. function via currying\n",
    "def maxNormReg(threshold, axes = 1, name = \"maxNormReg\", collection = \"max_norm\"):\n",
    "    \n",
    "    def maxNorm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm = threshold, axes = axes)\n",
    "        clippedWeights = tf.assign(weights, clipped, name = name)\n",
    "        tf.add_to_collection(collection, clippedWeights)\n",
    "        return None\n",
    "    \n",
    "    return maxNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Acc:  0.9 Test Acc:  0.8981\n",
      "10 Train Acc:  1.0 Test Acc:  0.9573\n",
      " \n",
      "FINAL ::  Train Acc:  0.94 Test Acc:  0.9684\n"
     ]
    }
   ],
   "source": [
    "# Reset the TF CG\n",
    "resetGraph()\n",
    "\n",
    "# Assign max norm reg. object\n",
    "reg = maxNormReg(1.0)\n",
    "\n",
    "# Build TF CG\n",
    "x = tf.placeholder(tf.float32, shape = [None, inputs], name = 'x')\n",
    "y = tf.placeholder(tf.int64, shape = [None], name = 'y')\n",
    "\n",
    "# This will let TF take care of creating and init'ing the weights and creating the bias\n",
    "with tf.name_scope(\"neuralNetEasy\"):\n",
    "    # He initializer\n",
    "    heInit = tf.contrib.layers.variance_scaling_initializer()\n",
    "    \n",
    "    # Note that we'll implement the max-norm reg. in the NN layers below via the 'kernel_regularizer' call\n",
    "    layer1 = tf.layers.dense(x, hidden1, name = \"hLayerOne\", activation = tf.nn.relu, \n",
    "                             kernel_initializer = heInit, kernel_regularizer = reg)\n",
    "    layer2 = tf.layers.dense(layer1, hidden2, name = \"hLayerTwo\", activation = tf.nn.relu, kernel_regularizer = reg)\n",
    "    yH = tf.layers.dense(layer2, outputs, name = \"yH\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = yH)\n",
    "    loss = tf.reduce_mean(entropy, name = \"loss\")\n",
    "    \n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    opt =  tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "    \n",
    "# Eval the model's accuracy\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(yH, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "clippedWeights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "# Execute the TF CG\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for i in range(mninst.train.num_examples // batchSize):\n",
    "            xBatch, yBatch = mninst.train.next_batch(batchSize)\n",
    "            sess.run([opt], feed_dict = {x: xBatch, y: yBatch})\n",
    "            # Max-norm weight clipping\n",
    "            sess.run(clippedWeights)\n",
    "        accTrain = accuracy.eval(feed_dict = {x: xBatch, y: yBatch})\n",
    "        accTest = accuracy.eval(feed_dict = {x: mninst.test.images, y: mninst.test.labels})\n",
    "        if e % 10 == 0:\n",
    "            print(e, \"Train Acc: \", accTrain, \"Test Acc: \", accTest)\n",
    "        \n",
    "    print(\" \")\n",
    "    print(\"FINAL :: \", \"Train Acc: \", accTrain, \"Test Acc: \", accTest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
